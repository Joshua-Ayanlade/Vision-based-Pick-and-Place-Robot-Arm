# Vision-based Pick-and-Place-Robot-Arm
## Overview
<p align="justify">
This project implements a vision-based pick-and-place robot arm using ROS. Three cameras are used, each serving a specific purpose: AR-tag detection, object pose estimation, and environment mapping for collision avoidance. The robot arm leverages MoveIt for trajectory planning and includes custom ROS service servers for image processing and object position computation with respect to the camera frame center. Modifications were made to the "onAttach" trigger conditions in the GazeboGraspFix.cpp file (created by <a href="https://github.com/JenniferBuehler/gazebo-pkgs">Jennifer Buehler</a>) to improve the chances of successful grasping. Using prompt from the user via the command terminal, the color of the boxes to be spawned to Gazebo are extracted and their corresponding optminzed HSV ranges were automatically generated using a ChatGPT request which enables flexible color-based object detection without manually tuning HSV values.
</p>

## ğŸ“Œ Key features:
- âœ… Vision-based control of a Universal Robots UR10 robotic arm for pick-and-place tasks.
- âœ… Gazebo simulation with scene cameras for AR-tag detection and MoveIt collision mapping (OctoMap).
- âœ… Custom Gazebo world file including the robot arm base stand, AR tags, and two scene cameras.
- âœ… Custom service server for tracking the current pose of the end effector, enabling seamless implementation of Cartesian trajectory motions.
- âœ… Custom service server for object center detection.
- âœ… Coordinate transformation of the camera frame for proper orientation of point cloud data in RViz.
- âœ… Custom **XACRO model** attaching an EGH gripper and mounting a camera to the gripper base frame for improved object tracking.
- âœ… Enhanced GazeboGraspFix for more reliable grasping.
- âœ… Custom chat gpt service for extracting color names from the user prompt and generating optimized HSV ranges for each color

---
## ğŸ“‚ ur10_robot_arm Package structure
```bash
ur10_robot_arm/
â”œâ”€â”€ config/  
â”‚   â””â”€â”€ joint_trajectory_controller.yaml   # Controller config for UR10 in Gazebo  
â”‚
â”œâ”€â”€ images/                                # Images for extracting HSV values of objects  
â”‚
â”œâ”€â”€ launch/  
â”‚   â”œâ”€â”€ ar_marker_detect.launch            # Detect AR tags & compute poses wrt *ur_base_link*  
â”‚   â”œâ”€â”€ target_pose_transform.launch       # Static transform of AR tag frames (linear & rotational)  
â”‚   â”œâ”€â”€ ur10_w_gripper.launch              # Spawn UR10 + gripper, controllers, camera TF, RViz  
â”‚   â””â”€â”€ ur10_w_gripper_combo.launch        # Combo: all above + MoveIt, box spawner, camera & EE servers  
â”‚
â”œâ”€â”€ models/  
â”‚   â”œâ”€â”€ ar_tag_generator.py                # Generate material files for AR markers  
â”‚   â””â”€â”€ sdf_config_generator.py            # Generate SDF config for AR markers  
â”‚
â”œâ”€â”€ scripts/  
â”‚   â”œâ”€â”€ add_box.py                         # Spawn boxes in Gazebo  
â”‚   â”œâ”€â”€ ar_tag.py                          # Main script for robotâ€“AR tag interaction
â”‚   â”œâ”€â”€ chat_gpt_server.py                 # Service: generate Max and Min HSV values of the requested color   
â”‚   â”œâ”€â”€ endEffectorPose_server.py          # Service Server: get end-effector pose
â”‚   â”œâ”€â”€ setup_color_hsv.py                 # Service Client: send user prompt to chatgpt server and place values in parameter server
â”‚   â””â”€â”€ stream_camera_server.py            # Service Server: detect object & compute camera deviation  
â”‚
â”œâ”€â”€ srv/  
â”‚   â”œâ”€â”€ chatPrompt.srv                     # Service: request prompt model color â†’ return color & HSV values 
â”‚   â”œâ”€â”€ endEffectorPose.srv                # Service: request pose â†’ return EE pose
â”‚   â””â”€â”€ frameDev.srv                       # Service: request color â†’ return (x,y) deviation  
â”‚
â”œâ”€â”€ world/  
â”‚   â””â”€â”€ ar_marker_chand.world              # Gazebo world: UR10 base stand, AR tags, scene cameras  
â”‚
â”œâ”€â”€ xacro/  
â”‚   â”œâ”€â”€ ur10_w_gripper.xacro               # UR10 + egh gripper, cameras, octomap  
â”‚   â””â”€â”€ multi_kinect.xacro                 # Define multiple Kinect camera models  
â”‚
â”œâ”€â”€ CMakeLists.txt  
â””â”€â”€ package.xml

moveit/
â”œâ”€â”€ config/    #trajectory controller configuration and sensors files 
â”œâ”€â”€ launch/    #move_group and controllers launch files
â”œâ”€â”€ CMakeLists.txt        
â””â”€â”€ package.xml

ur_description/
â”œâ”€â”€ meshes/    #mesh files for the gripper base and finger
â”œâ”€â”€ urdf/    #xacro file for the gripper
â”œâ”€â”€ CMakeLists.txt        
â””â”€â”€ package.xml

egh_gripper_description/
â”œâ”€â”€ meshes/    #mesh files for the gripper base and finger
â”œâ”€â”€ urdf/    #xacro file for the gripper
â”œâ”€â”€ CMakeLists.txt        
â””â”€â”€ package.xml

grasps_fix_plugin/ #modified grasps fix plugin file for improved grasping

```

---
## Dependencies
- rospy
- std_msgs
- openCV
- message_generation
- gripper_action_controller
- moveit_package
- joint_state_publisher
- joint_state_publisher_gui
- robot_state_publisher
- rviz
- tf2_ros
- xacro

---
## ğŸ› ï¸ Installation & Build
1.  Create a Catkin Workspace (if you haven't already):
    ```bash
    mkdir -p ~/catkin_ws/src
    cd ~/catkin_ws/
    catkin_make
    source devel/setup.bash
    ```
    
2. Clone the repository into your ROS workspace:  
   ```bash
    cd ~/catkin_ws/src
    git clone https://github.com/Joshua-Ayanlade/Vision-based-Pick-and-Place-Robot-Arm.git
    ```

3. Install Dependencies:
   ```bash
    cd ~/catkin_ws
    rosdep install --from-paths src --ignore-src -y
    ```

4. Build the Package:
   ```bash
    cd ~/catkin_ws
    catkin_make
    source devel/setup.bash
    ```

---
## âš™ï¸ Usage
   
1. Start the ROS Master:  
   `roscore`
   
2. Start the chat gpt server:  
   `rosrun ur10_robot_arm chat_gpt_server.py`

3. Start the setup color hsv service client with input request as argument:  
   `rosrun ur10_robot_arm setup_color_hsv.py 'spawn red and blue box in gazebo'`
   
4. Launch the Gazebo world, spawn the robot arm, load the box model, start the controllers, launch MoveIt, initialize pose transformation, start the AR-tag detector, and run the camera and end-effector pose servers:  
   `roslaunch ur10_robot_arm ur10_w_gripper_combo.launch`

5. Execute the pick-and-place routine and specify the AR tag from which you want to pick the object. Follow the prompts afterwards:
   
   `roslaunch ur10_robot_arm ar_tag.py 1`&nbsp;&nbsp;&nbsp;&nbsp;# *Robot arm moves towards AR Tag 1* 


---
## ğŸ¥ Demo
![UR10+arm_Pick+'n'+Place (1)](https://github.com/user-attachments/assets/088f6231-3360-416a-a6d1-d8fd38d40a41)

---

## ğŸ¥ Demo with ChatGPT api

![ur10arm](https://github.com/user-attachments/assets/206abb86-eaa5-4aa7-b879-3d9f1b1bbb46)

---

## ğŸ“š Resources
- *A Systematic Approach to Learning Robot Programming with ROS* â€” Wyatt S. Newman  
- *Programming Robots with ROS* â€” Morgan Quigley et al.  
- *ROS by Example: Packages and Programs for Advanced Robot Behaviors (Vol. 2)* â€” R. Patrick Goebel  
- [ROS Wiki](https://wiki.ros.org)  
- [REP-103 Standard](https://www.ros.org/reps/rep-0103.html)  
- [Jennifer Buehlerâ€™s general-message-pkgs](https://github.com/JenniferBuehler/general-message-pkgs.git) 
- [Jennifer Buehlerâ€™s gazebo-pkgs](https://github.com/JenniferBuehler/gazebo-pkgs.git) 
- [Moveit](github.com/ros-planning/moveit)
  
## Acknowledgement
- ur_description package authored by **Wim Meeussen et al**
- egh_gripper_description package authored by **Jane Done**
